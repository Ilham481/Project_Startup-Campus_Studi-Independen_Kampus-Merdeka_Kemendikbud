{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tugas 7 Bixby Ilham Habibullah"
      ],
      "metadata": {
        "id": "SwRm_LXryiOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlu2HmgxymKO",
        "outputId": "f7b158d7-c169-4d5b-f34d-f715d03606f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"Cristiano Ronaldo came off the bench to earn Manchester United a hard-fought 2-1 victory at Everton in the Premier League on Sunday, taking his career goal tally to 700 in the process. Just as United did last weekend in their derby mauling at the hands of local rivals Manchester City, they again found themselves behind early on at Goodison Park after Alex Iwobi curled a sublime strike into the net from 20 metres\"\n",
        "\n",
        "# sent_tokenize (Separated by sentence)\n",
        "sentences = sent_tokenize(example_text)\n",
        "print(sentences)\n",
        "\n",
        "##word_tokenize (Separated by words)\n",
        "words = word_tokenize(example_text)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYRI_4E7y9H5",
        "outputId": "4d733761-9e6f-45eb-96f6-c6a2ee511836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cristiano Ronaldo came off the bench to earn Manchester United a hard-fought 2-1 victory at Everton in the Premier League on Sunday, taking his career goal tally to 700 in the process.', 'Just as United did last weekend in their derby mauling at the hands of local rivals Manchester City, they again found themselves behind early on at Goodison Park after Alex Iwobi curled a sublime strike into the net from 20 metres']\n",
            "['Cristiano', 'Ronaldo', 'came', 'off', 'the', 'bench', 'to', 'earn', 'Manchester', 'United', 'a', 'hard-fought', '2-1', 'victory', 'at', 'Everton', 'in', 'the', 'Premier', 'League', 'on', 'Sunday', ',', 'taking', 'his', 'career', 'goal', 'tally', 'to', '700', 'in', 'the', 'process', '.', 'Just', 'as', 'United', 'did', 'last', 'weekend', 'in', 'their', 'derby', 'mauling', 'at', 'the', 'hands', 'of', 'local', 'rivals', 'Manchester', 'City', ',', 'they', 'again', 'found', 'themselves', 'behind', 'early', 'on', 'at', 'Goodison', 'Park', 'after', 'Alex', 'Iwobi', 'curled', 'a', 'sublime', 'strike', 'into', 'the', 'net', 'from', '20', 'metres']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPyAKnIpzVNy",
        "outputId": "1b9a0d6a-24b3-4fc2-ac5a-169dda769d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "text = \"Cristiano Ronaldo came off the bench to earn Manchester United a hard-fought 2-1 victory at Everton in the Premier League on Sunday, taking his career goal tally to 700 in the process. Just as United did last weekend in their derby mauling at the hands of local rivals Manchester City, they again found themselves behind early on at Goodison Park after Alex Iwobi curled a sublime strike into the net from 20 metres\"\n",
        "words = word_tokenize(text)\n",
        "words_without_stopwords = [word for word in words if word not in stopwords.words('english')]\n",
        "print(words_without_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jIzCgz2zgjt",
        "outputId": "df55789d-73a0-40c3-8420-1d49854de523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cristiano', 'Ronaldo', 'came', 'bench', 'earn', 'Manchester', 'United', 'hard-fought', '2-1', 'victory', 'Everton', 'Premier', 'League', 'Sunday', ',', 'taking', 'career', 'goal', 'tally', '700', 'process', '.', 'Just', 'United', 'last', 'weekend', 'derby', 'mauling', 'hands', 'local', 'rivals', 'Manchester', 'City', ',', 'found', 'behind', 'early', 'Goodison', 'Park', 'Alex', 'Iwobi', 'curled', 'sublime', 'strike', 'net', '20', 'metres']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer() ## Create object for lemmatizer\n",
        "example_words = ['Cristiano Ronaldo', 'came', 'off', 'the', 'bench', 'to', 'earn', 'Manchester United', 'a', 'hard-fought', '2-1', 'victory', 'at', 'Everton', 'in', 'the', 'Premier', 'League', 'on', 'Sunday', 'taking', 'his', 'career', 'goal', 'tally', 'to', '700', 'in', 'the', 'process', 'Just', 'as', 'United', 'did', 'last', 'weekend', 'in', 'their', 'derby', 'mauling', 'at', 'the', 'hands', 'of', 'local', 'rivals', 'Manchester City', 'they', 'again', 'found', 'themselves', 'behind', 'early', 'on', 'at', 'Goodison', 'Park', 'after', 'Alex Iwobi', 'curled', 'a', 'sublime', 'strike', 'into', 'the', 'net', 'from', '20', 'metres']\n",
        "for w in example_words:\n",
        "    print(lemmatizer.lemmatize(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5YbZ4Vuz0zB",
        "outputId": "9ab1463d-f5a4-4492-af2b-e73a47c35905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cristiano Ronaldo\n",
            "came\n",
            "off\n",
            "the\n",
            "bench\n",
            "to\n",
            "earn\n",
            "Manchester United\n",
            "a\n",
            "hard-fought\n",
            "2-1\n",
            "victory\n",
            "at\n",
            "Everton\n",
            "in\n",
            "the\n",
            "Premier\n",
            "League\n",
            "on\n",
            "Sunday\n",
            "taking\n",
            "his\n",
            "career\n",
            "goal\n",
            "tally\n",
            "to\n",
            "700\n",
            "in\n",
            "the\n",
            "process\n",
            "Just\n",
            "a\n",
            "United\n",
            "did\n",
            "last\n",
            "weekend\n",
            "in\n",
            "their\n",
            "derby\n",
            "mauling\n",
            "at\n",
            "the\n",
            "hand\n",
            "of\n",
            "local\n",
            "rival\n",
            "Manchester City\n",
            "they\n",
            "again\n",
            "found\n",
            "themselves\n",
            "behind\n",
            "early\n",
            "on\n",
            "at\n",
            "Goodison\n",
            "Park\n",
            "after\n",
            "Alex Iwobi\n",
            "curled\n",
            "a\n",
            "sublime\n",
            "strike\n",
            "into\n",
            "the\n",
            "net\n",
            "from\n",
            "20\n",
            "metre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words"
      ],
      "metadata": {
        "id": "aXxNV2QK2pru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['Cristiano Ronaldo', 'came', 'off', 'the', 'bench', 'to', 'earn', 'Manchester United', 'a', 'hard-fought', '2-1', 'victory', 'at', 'Everton', 'in', 'the', 'Premier', 'League', 'on', 'Sunday', 'taking', 'his', 'career', 'goal', 'tally', 'to', '700', 'in', 'the', 'process', 'Just', 'as', 'United', 'did', 'last', 'weekend', 'in', 'their', 'derby', 'mauling', 'at', 'the', 'hands', 'of', 'local', 'rivals', 'Manchester City', 'they', 'again', 'found', 'themselves', 'behind', 'early', 'on', 'at', 'Goodison', 'Park', 'after', 'Alex Iwobi', 'curled', 'a', 'sublime', 'strike', 'into', 'the', 'net', 'from', '20', 'metres']\n",
        "corpus = []\n",
        "for sent in sentences:\n",
        "    words  = word_tokenize(sent)\n",
        "    texts = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('indonesian'))]\n",
        "    text = ' '.join(texts)\n",
        "    corpus.append(text)\n",
        "print(corpus)   #### Cleaned Data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer() ## Creating Object for CountVectorizer\n",
        "word_counts = cv.fit_transform(corpus).toarray()\n",
        "print(word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4meJGx8H2uaQ",
        "outputId": "d736f090-c529-4cc5-8904-13d0bbb0724e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cristiano Ronaldo', 'came', 'off', 'the', 'bench', 'to', 'earn', 'Manchester United', 'a', 'hard-fought', '2-1', 'victory', 'at', 'Everton', 'in', 'the', 'Premier', 'League', 'on', 'Sunday', 'taking', 'his', 'career', 'goal', 'tally', 'to', '700', 'in', 'the', 'process', 'Just', 'a', 'United', 'did', 'last', 'weekend', 'in', 'their', 'derby', 'mauling', 'at', 'the', 'hand', 'of', 'local', 'rival', 'Manchester City', 'they', 'again', 'found', 'themselves', 'behind', 'early', 'on', 'at', 'Goodison', 'Park', 'after', 'Alex Iwobi', 'curled', 'a', 'sublime', 'strike', 'into', 'the', 'net', 'from', '20', 'metre']\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# five texts with labeled topic\n",
        "text_satu = \"Health\"\n",
        "text_dua = \"Sport\"\n",
        "text_tiga = \"Finance\"\n",
        "\n",
        "texts = [text_satu, text_dua, text_tiga]\n",
        "bow_keys = []\n",
        "corpus_texts = []\n",
        "for text in texts:\n",
        "    words  = word_tokenize(text)\n",
        "    texts = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('indonesian'))]\n",
        "    bow_keys += texts\n",
        "    text = ' '.join(texts)\n",
        "    corpus_texts.append(text)\n",
        "bow_keys = set(bow_keys)\n",
        "print(bow_keys)   #### Cleaned Data\n",
        "print(corpus_texts)   #### Cleaned Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKzp903a3I3O",
        "outputId": "37541cbb-000a-42d8-ab25-68a6c96d4e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Health', 'Sport', 'Finance'}\n",
            "['Health', 'Sport', 'Finance']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer() ## Creating Object for CountVectorizer\n",
        "bow_vectors = cv.fit_transform(corpus_texts).toarray()\n",
        "print(bow_vectors)\n",
        "print(len(bow_vectors[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URXWDpeV3neq",
        "outputId": "99a81652-46a3-4144-9fac-56649785c3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0]\n",
            " [0 0 1]\n",
            " [1 0 0]]\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the BoW vectors\n",
        "bow_texts_norm = []\n",
        "for bow in bow_vectors:\n",
        "  length = (sum(i*i for i in bow)) ** 0.5\n",
        "  bow_norm = bow / length\n",
        "  bow_texts_norm.append(bow_norm)\n",
        "\n",
        "# Compute similarity using dot product\n",
        "similarity_vector = []\n",
        "bow_norm_query = bow_texts_norm[2]\n",
        "for bow in bow_texts_norm[:3]:\n",
        "  similarity_vector.append(sum(i*j for i,j in zip(bow,bow_norm_query)))\n",
        "print(similarity_vector)\n",
        "\n",
        "# Find the highest similarity\n",
        "id_max_sim = similarity_vector.index(max(similarity_vector))\n",
        "if (id_max_sim == 0):\n",
        "  print (\"The query text is classified as: Health\")\n",
        "elif (id_max_sim == 1):\n",
        "  print (\"The query text is classified as: Sport\")\n",
        "elif (id_max_sim == 2):\n",
        "  print (\"The query text is classified as: Finance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNFKWFVJ3pe3",
        "outputId": "6231b7ab-a25d-48c7-cfa0-9d090175516f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 1.0]\n",
            "The query text is classified as: Finance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HomeWork 3"
      ],
      "metadata": {
        "id": "igo6tXCG4G-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jawab : Pada dasarnya teknologi NLP memang masih cukup sulit untuk digunakan dibandingkan dengan metode lainnya dari teknologi AI. Penerapan conversational artificial intelligence pun semakin berkembang pesat. Salah satunya adalah penerapan sentiment analysis untuk mengetahui arah pembicaraan. Akan tetapi jika bisa mengembangkan teknologi ini dengan baik, akan memberikan banyak manfaat khususnya dalam dunia bisnis. Dengan menggunakan teknologi Natural Language Processing ini, berbagai pekerjaan manusia menjadi lebih mudah dan cepat untuk dikerjakan oleh komputer. Meskipun demikian, manusia juga harus lebih kreatif dan inovatif agar tidak dikalahkan oleh teknologi komputer seperti ini."
      ],
      "metadata": {
        "id": "HCOG9n2d4Lyp"
      }
    }
  ]
}